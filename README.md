
## ğŸ›º ì°¨ëŸ‰ ë²ˆí˜¸íŒ ì¸ì‹ ëª¨ë¸, Rasp ìµœì í™” 
# ìµœì¢… ê²°ê³¼ë¬¼

https://github.com/user-attachments/assets/3fee0053-b047-47ba-b2ff-e11905a71c74




- í”„ë¡œì íŠ¸ì— ì‚¬ìš©í•  ì´ë¯¸ì§€ (ëŒ€í‘œ ì˜ˆì‹œ)
![1](https://github.com/user-attachments/assets/6c8f4fef-68ec-4086-b8ec-1b484e1dd87e)



## Tesseract OCR ì‚¬ìš© text recognition with YOLOv3



https://github.com/user-attachments/assets/0bd5e550-76a5-4df9-80a5-5d9af3f3e9ba



- **ì½”ë“œ ìš”ì•½ ì„¤ëª…**
    - ì°¨ëŸ‰ ë²ˆí˜¸íŒì„ ê°ì§€í•˜ê³ , í•´ë‹¹ ë²ˆí˜¸íŒì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¸ì‹í•˜ëŠ” Python ì½”ë“œì´ë‹¤. ì´ ì½”ë“œëŠ” OpenCVë¥¼ í™œìš©í•˜ì—¬ ê°ì²´ íƒì§€ ë° ì´ë¯¸ì§€ ì²˜ë¦¬ ì‘ì—…ì„ ìˆ˜í–‰í•˜ë©°, YOLOì™€ EAST ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì°¨ëŸ‰ ë° í…ìŠ¤íŠ¸ ê°ì§€ë¥¼ êµ¬í˜„í•œë‹¤
    - ë²ˆí˜¸íŒ í…ìŠ¤íŠ¸ëŠ” Tesseract OCRì„ í†µí•´ ì½ëŠ”ë‹¤

```python
import cv2
import numpy as np
from imutils.object_detection import non_max_suppression
import pytesseract

min_confidence = 0.5
file_name = "image/plate_01.jpg"

east_decorator = 'frozen_east_text_detection.pb'

frame_size = 320
padding = 0.05

# Load Yolo
net = cv2.dnn.readNet("yolo/yolov3.weights", "yolo/yolov3.cfg")
layer_names = net.getLayerNames()
output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]
    
def carROI(image):
    height, width, channels = image.shape
    # Detecting objects
    blob = cv2.dnn.blobFromImage(img, 0.00392, (416, 416), (0, 0, 0), True, crop=False)
    
    net.setInput(blob)
    outs = net.forward(output_layers)

    # Showing informations on the screen
    confidences = []
    boxes = []
    img_cars = []

    for out in outs:
        for detection in out:
            scores = detection[5:]
            class_id = np.argmax(scores)
            confidence = scores[class_id]
            # Filter only 'car'
            if class_id == 2 and confidence > min_confidence:
                # Object detected
                center_x = int(detection[0] * width)
                center_y = int(detection[1] * height)
                w = int(detection[2] * width)
                h = int(detection[3] * height)

                # Rectangle coordinates
                x = int(center_x - w / 2)
                y = int(center_y - h / 2)

                boxes.append([x, y, w, h])
                confidences.append(float(confidence))
                

    indexes = cv2.dnn.NMSBoxes(boxes, confidences, min_confidence, 0.4)
    for i in range(len(boxes)):
        if i in indexes:
            x, y, w, h = boxes[i]
            img_cars.append(image[y:y+h, x:x+w])
            return (boxes[i], image[y:y+h, x:x+w])

def textROI(image):
    # load the input image and grab the image dimensions
    orig = image.copy()
    (origH, origW) = image.shape[:2]
 
    # set the new width and height and then determine the ratio in change
    rW = origW / float(frame_size)
    rH = origH / float(frame_size)
    newW = int(origW / rH)
    center = int(newW / 2)
    start = center - int(frame_size / 2)
 
    # resize the image and grab the new image dimensions
    image = cv2.resize(image, (newW, frame_size))  
    scale_image = image[0:frame_size, start:start+frame_size]
    (H, W) = scale_image.shape[:2]

    cv2.imshow("orig", orig)
    cv2.imshow("resize", image)
    cv2.imshow("scale_image", scale_image)
    
    # define the two output layer names for the EAST detector model 
    layerNames = [
            "feature_fusion/Conv_7/Sigmoid",
            "feature_fusion/concat_3"]
    
    # load the pre-trained EAST text detector
    net = cv2.dnn.readNet(east_decorator)

    # construct a blob from the image 
    blob = cv2.dnn.blobFromImage(image, 1.0, (frame_size, frame_size),
            (123.68, 116.78, 103.94), swapRB=True, crop=False)
    net.setInput(blob)
    (scores, geometry) = net.forward(layerNames)

    (numRows, numCols) = scores.shape[2:4]
    rects = []
    confidences = []

    # loop over the number of rows
    for y in range(0, numRows):
        # extract the scores (probabilities)
        scoresData = scores[0, 0, y]
        xData0 = geometry[0, 0, y]
        xData1 = geometry[0, 1, y]
        xData2 = geometry[0, 2, y]
        xData3 = geometry[0, 3, y]
        anglesData = geometry[0, 4, y]

        # loop over the number of columns
        for x in range(0, numCols):

                if scoresData[x] < min_confidence:
                        continue

                (offsetX, offsetY) = (x * 4.0, y * 4.0)

                angle = anglesData[x]
                cos = np.cos(angle)
                sin = np.sin(angle)

                h = xData0[x] + xData2[x]
                w = xData1[x] + xData3[x]

                endX = int(offsetX + (cos * xData1[x]) + (sin * xData2[x]))
                endY = int(offsetY - (sin * xData1[x]) + (cos * xData2[x]))
                startX = int(endX - w)
                startY = int(endY - h)

                rects.append((startX, startY, endX, endY))
                confidences.append(scoresData[x])
    
    # apply non-maxima suppression 
    boxes = non_max_suppression(np.array(rects), probs=confidences)

    # initialize the list of results
    results = []

    # loop over the bounding boxes
    for (startX, startY, endX, endY) in boxes:

            startX = int(startX * rW)
            startY = int(startY * rH)
            endX = int(endX * rW)
            endY = int(endY * rH)

            dX = int((endX - startX) * padding)
            dY = int((endY - startY) * padding)

            startX = max(0, startX - dX)
            startY = max(0, startY - dY)
            endX = min(origW, endX + (dX * 2))
            endY = min(origH, endY + (dY * 2))

            # extract the actual padded ROI
            return ([startX, startY, endX, endY], orig[startY:endY, startX:endX])

	
def textRead(image):
    # apply Tesseract v4 to OCR 
    config = ("-l eng --oem 1 --psm 7")
    text = pytesseract.image_to_string(image, config=config)
    # display the text OCR'd by Tesseract
    print("OCR TEXT : {}\n".format(text))
    
    # strip out non-ASCII text 
    text = "".join([c if c.isalnum() else "" for c in text]).strip()
    print("Alpha numeric TEXT : {}\n".format(text))
    return text

def processROI(image):
    # hsv transform - value = gray image
    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
    hue, saturation, value = cv2.split(hsv)

    cv2.imshow("value", value)
    # kernel to use for morphological operations
    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))

    # applying topHat operations
    topHat = cv2.morphologyEx(value, cv2.MORPH_TOPHAT, kernel)

    # applying blackHat operations
    blackHat = cv2.morphologyEx(value, cv2.MORPH_BLACKHAT, kernel)

    # add and subtract between morphological operations
    add = cv2.add(value, topHat)
    subtract = cv2.subtract(add, blackHat)

    # applying gaussian blur on subtract image
    blur = cv2.GaussianBlur(subtract, (5, 5), 0)
    cv2.imshow("blur", blur)

    # thresholding
    thresh = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]
    cv2.imshow("thresh", thresh)    
    return image

# Loading image
img = cv2.imread(file_name)
img_copy = img.copy()
([x, y, w, h], car_image) = carROI(img)

([startX, startY, endX, endY], text_image) = textROI(car_image)

process_image = processROI(text_image)

text = textRead(process_image)

cv2.rectangle(img_copy, (x+startX, y+startY), (x+endX, y+endY), (0, 255, 0), 2)

cv2.putText(img_copy, text, (x+startX, y+startY-10),
    cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 0, 255), 3)

# show the output image
cv2.imshow("OCR Text Recognition : "+text, img_copy)

cv2.waitKey(0)
cv2.destroyAllWindows()

```

### 1. **ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸**

- **`cv2`**: OpenCV ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ ì´ë¯¸ì§€ ì²˜ë¦¬ ë° ê°ì²´ íƒì§€ë¥¼ ë‹´ë‹¹
- **`numpy`**: ë°°ì—´ ë° ìˆ˜í•™ì  ê³„ì‚°ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
- **`imutils.object_detection`**: ë¹„ìµœëŒ€ ì–µì œ(non-max suppression)ë¥¼ ì ìš©í•˜ì—¬ ê°ì²´ íƒì§€ì˜ ì •í™•ë„ë¥¼ ë†’ì„
    - ***Non-Max Suppressionì€ ë™ì¼í•œ ê°ì²´ì— ëŒ€í•´ ë‹¤ìˆ˜ì˜ ê²¹ì¹˜ëŠ” ê²½ê³„ ìƒìê°€ ì˜ˆì¸¡ë˜ëŠ” ìƒí™©ì—ì„œ, ê°€ì¥ ì‹ ë¢°ë„ê°€ ë†’ì€ í•˜ë‚˜ì˜ ê²½ê³„ ìƒìë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ ê²½ê³„ ìƒìë¥¼ ì œê±°í•˜ëŠ” ê¸°ë²•***
- **`pytesseract`**: OCR(Optical Character Recognition)ì„ ìœ„í•´ ì‚¬ìš©
    - ***ê´‘í•™ ë¬¸ì ì¸ì‹(Optical Character Recognition)ì€ ì´ë¯¸ì§€ ë˜ëŠ” ìŠ¤ìº”ëœ ë¬¸ì„œì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ëŠ” ê¸°ìˆ ì´ë‹¤. ì»´í“¨í„°ê°€ ì‚¬ëŒì´ ì‘ì„±í•˜ê±°ë‚˜ ì¸ì‡„í•œ í…ìŠ¤íŠ¸ë¥¼ ì½ê³  ì´ë¥¼ ë””ì§€í„¸ ë°ì´í„°(í…ìŠ¤íŠ¸ íŒŒì¼)ë¡œ ë³€í™˜í•  ìˆ˜ ìˆê²Œ í•œë‹¤***
    - ***ì´ë¯¸ì§€ ì „ì²˜ë¦¬ â†’ ë¬¸ì ì˜ì—­ ê°ì§€ â†’ ë¬¸ì ë¶„ë¦¬ â†’ ë¬¸ì ì¸ì‹ â†’ ê²°ê³¼ ì¶œë ¥***
    - ***TesseractëŠ” ê°€ì¥ ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” OCR ì˜¤í”ˆì†ŒìŠ¤ ì—”ì§„ ì¤‘ í•˜ë‚˜, Pythonì˜ pytesseract ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” Tesseract ì—”ì§„ì„ íŒŒì´ì¬ì—ì„œ ì‰½ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ë„ì™€ì¤€ë‹¤***

### 2. **ëª¨ë¸ ì´ˆê¸°í™”**

- **YOLO ëª¨ë¸ ì´ˆê¸°í™”**:
    - ê°€ì¤‘ì¹˜(`yolov3.weights`)ì™€ ì„¤ì • íŒŒì¼(`yolov3.cfg`) ë¡œë“œ
    - ì¶œë ¥ ë ˆì´ì–´ ì´ë¦„ì„ ì¶”ì¶œí•˜ì—¬ ëª¨ë¸ ì¶œë ¥ êµ¬ì„±
- **EAST ëª¨ë¸ ì´ˆê¸°í™”**:
    - ì‚¬ì „ í•™ìŠµëœ `frozen_east_text_detection.pb` ëª¨ë¸ ë¡œë“œ

### 3. **í•¨ìˆ˜ ì •ì˜**

- **`carROI(image)`**:
    - **ì°¨ëŸ‰ ê°ì²´**ë¥¼ ê°ì§€í•˜ì—¬ í•´ë‹¹ ì˜ì—­ì„ ë°˜í™˜
    - YOLOë¥¼ ì‚¬ìš©í•˜ì—¬ ê°ì²´ ê°ì§€ ìˆ˜í–‰
    - ê°ì§€ëœ 'ì°¨ëŸ‰' í´ë˜ìŠ¤ë§Œ í•„í„°ë§í•˜ê³  **ë¹„ìµœëŒ€ ì–µì œ ì ìš©(non-max suppression)**
- **`textROI(image)`**:
    - **ë²ˆí˜¸íŒ í…ìŠ¤íŠ¸**ê°€ ìˆì„ ë²•í•œ ì˜ì—­ì„ **EAST ëª¨ë¸**ì„ ì‚¬ìš©í•˜ì—¬ íƒì§€
        - **EAST ëª¨ë¸ì€ Efficient and Accurate Scene Text Detectionì˜ ì•½ì**
        - **ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ë¹ ë¥´ê³  ì •í™•í•˜ê²Œ íƒì§€í•˜ê¸° ìœ„í•œ ë”¥ëŸ¬ë‹ ê¸°ë°˜ í…ìŠ¤íŠ¸ ê²€ì¶œ ëª¨ë¸**
    - ê°ì§€ëœ í…ìŠ¤íŠ¸ ì˜ì—­ì„ **ROI**ë¡œ ë°˜í™˜
- **`textRead(image)`**:
    - Tesseract OCRì„ í†µí•´ ROIì˜ í…ìŠ¤íŠ¸ë¥¼ ì¸ì‹
    - OCR ê²°ê³¼ë¥¼ ì•ŒíŒŒë²³ ë° ìˆ«ìë§Œ í¬í•¨í•œ í…ìŠ¤íŠ¸ë¡œ ì •ì œ
- **`processROI(image)`**:
    - ì´ë¯¸ì§€ì˜ ë²ˆí˜¸íŒ í…ìŠ¤íŠ¸ë¥¼ ì„ ëª…í•˜ê²Œ í•˜ê¸° ìœ„í•œ ì „ì²˜ë¦¬ ì‘ì—…
    - HSV ë³€í™˜, top-hat, black-hat ì—°ì‚°, ë¸”ëŸ¬ë§, ì´ì§„í™” ì ìš©

### 4. **ë©”ì¸ ì½”ë“œ**

- ì´ë¯¸ì§€ íŒŒì¼ ë¡œë“œ
- **`carROI()`**: ì°¨ëŸ‰ ROI ì¶”ì¶œ (ROIëŠ” Region of interestì˜ ì•½ì)
- **`textROI()`**: ë²ˆí˜¸íŒ ROI ì¶”ì¶œ


<img width="455" alt="3" src="https://github.com/user-attachments/assets/31de77d7-2ffd-4658-8141-5649d961ab9d" />
<img width="417" alt="4" src="https://github.com/user-attachments/assets/b5bec913-9f53-499d-89c8-7d7db0fc9821" />


    
- **`processROI()`**: ROI ì´ë¯¸ì§€ ì „ì²˜ë¦¬

<img width="420" alt="5" src="https://github.com/user-attachments/assets/1d97f4ac-d0c5-4287-9809-c877b0f8188c" />
<img width="418" alt="6" src="https://github.com/user-attachments/assets/b9d79293-78e4-48f6-869b-8cce61915e89" />
<img width="419" alt="7" src="https://github.com/user-attachments/assets/afa08b2d-2a6d-47c4-8364-049d64bc4e01" />

    

- **`textRead()`**: OCRë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
- ì¶”ì¶œëœ í…ìŠ¤íŠ¸ë¥¼ ì›ë³¸ ì´ë¯¸ì§€ì— ì‹œê°ì ìœ¼ë¡œ í‘œì‹œ

![9](https://github.com/user-attachments/assets/8c1a2461-ab99-4a40-b016-a6597a6b9618)
<img width="692" alt="8" src="https://github.com/user-attachments/assets/a5579c98-3a52-461a-ba5e-5c7830155b0e" />


    

## ProcessROIë¡œ ë²ˆí˜¸íŒ ì˜ì—­ ì§ì ‘ ì¶”ì¶œ, text recognitionì€ Tesseract


https://github.com/user-attachments/assets/cbaabb76-4f89-43b1-a3e7-2f766165d1fb




- **ì½”ë“œ ìš”ì•½ ì„¤ëª…**
    - ì°¨ëŸ‰ ë²ˆí˜¸íŒ ì˜ì—­ì—ì„œ OCR(ê´‘í•™ ë¬¸ì ì¸ì‹)ì„ ìˆ˜í–‰

```python
import cv2
import numpy as np
import imutils
import math
import pytesseract

min_confidence = 0.5
file_name = "image/plate_01.jpg"

frame_size = 320
margin = 0
    
def processROI(image):
    # hsv transform - value = gray image
    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
    hue, saturation, value = cv2.split(hsv)
    #cv2.imshow('gray', value)
    
    # kernel to use for morphological operations
    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))

    # applying topHat/blackHat operations
    topHat = cv2.morphologyEx(value, cv2.MORPH_TOPHAT, kernel)
    blackHat = cv2.morphologyEx(value, cv2.MORPH_BLACKHAT, kernel)

    # add and subtract between morphological operations
    add = cv2.add(value, topHat)
    subtract = cv2.subtract(add, blackHat)

    # applying gaussian blur on subtract image
    blur = cv2.GaussianBlur(subtract, (5, 5), 0)

    # thresholding
    thresh = cv2.adaptiveThreshold(blur, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, 19, 9)

    # inverse black plate to white background
    invert = cv2.bitwise_not(value)
    
    # cv2.findCountours() function changed from OpenCV3 to OpenCV4: now it have only two parameters instead of 3
    cv2MajorVersion = cv2.__version__.split(".")[0]
    # check for contours on thresh
    if int(cv2MajorVersion) >= 4:
        contours, hierarchy = cv2.findContours(thresh, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)
    else:
        imageContours, contours, hierarchy = cv2.findContours(thresh, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)
    
    # get height and width
    height, width = thresh.shape

    # create a numpy array with shape given by threshed image value dimensions
    imageContours = np.zeros((height, width, 3), dtype=np.uint8)

    left_border = int(width * 0.3)
    right_border = int(width * 0.8)
    top_border = int(height * 0.5)
    bottom_border = int(height * 0.8)
    cv2.line(imageContours, (0, top_border), (width, top_border), (0, 255, 255), 2)
    cv2.line(imageContours, (0, bottom_border), (width, bottom_border), (0, 255, 255), 2)
    cv2.line(imageContours, (left_border, 0), (left_border, height), (0, 255, 255), 2)
    cv2.line(imageContours, (right_border, 0), (right_border, height), (0, 255, 255), 2)
    
    plateROI = invert[top_border:bottom_border, left_border:right_border]
    plateX = left_border
    plateW = right_border - left_border
    plateY = top_border
    plateH = bottom_border - top_border
    # Sort by area and filter top 10 
    contours = sorted(contours, key = cv2.contourArea, reverse = True)[:10]

    # loop to check if any (possible) char is found
    for i in range(0, len(contours)):
        # check which has a rectangle shape contour with four sides and closed figure
        peri = cv2.arcLength(contours[i], True)
        approx = cv2.approxPolyDP(contours[i], 0.018 * peri, True)
        boundingRect = cv2.boundingRect(contours[i])
        [x, y, w, h] = boundingRect
        rectArea = x * y
        if (rectArea > 100 and x > 10 and y > 10 and len(approx) == 4):
            # draw contours based on actual found contours of thresh image
            cv2.drawContours(imageContours, contours, i, (255, 255, 255))

            if (x > left_border and y > top_border and x+w < right_border and y+h < bottom_border):
                plateROI = invert[y-margin:y+h+margin, x-margin:x+w+margin]
                plateX = x
                plateW = w
                plateY = y
                plateH = h
                break
            
    cv2.imshow("Plate Candiates Contours", imageContours)
    cv2.imshow("Plate ROI", plateROI)
    
    return ([plateX, plateY, plateW, plateH], plateROI)

	
def textRead(image):
    # apply Tesseract v4 to OCR 
    config = ("-l eng --oem 3 --psm 12")
    text = pytesseract.image_to_string(image, config=config)
    # display the text OCR'd by Tesseract
    print("OCR TEXT : {}\n".format(text))
    
    # strip out non-ASCII text 
    text = "".join([c if c.isalnum() else "" for c in text]).strip()
    print("Alpha numeric TEXT : {}\n".format(text))
    return text

# Loading image
img = cv2.imread(file_name)
img_copy = img.copy()
#([x, y, w, h], car_image) = carROI(img)
#([startX, startY, endX, endY], text_image) = textROI(car_image)

([x, y, w, h], process_image) = processROI(img)

text = textRead(process_image)

cv2.rectangle(img_copy, (x-margin, y-margin), (x+w+margin, y+h+margin), (0, 255, 0), 2)
cv2.putText(img_copy, text, (x, y-margin-10),
    cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 0, 255), 3)

# show the output image
cv2.imshow("OCR Text Recognition : "+text, img_copy)

cv2.waitKey(0)
cv2.destroyAllWindows()

```

- **ì•ì„  ì½”ë“œì™€ì˜ ì°¨ë³„ì **
    - processROI êµ¬í˜„ ì°¨ì´
        - ìƒˆë¡œìš´ ì½”ë“œì—ì„œëŠ” processROIê°€ ì°¨ëŸ‰ì˜ ë²ˆí˜¸íŒ ì˜ì—­ì„ ì§ì ‘ ì¶”ì¶œí•œë‹¤ (ì•ì—ì„œëŠ” textROIê°€ í•´ë‹¹ ì—­í• )
        - ì´ì „ ì½”ë“œì—ì„œëŠ” YOLO ëª¨ë¸ì„ ì‚¬ìš©í•´ ì°¨ëŸ‰ì„ ê°ì§€í•œ í›„ EASTë¥¼ í†µí•´ í…ìŠ¤íŠ¸ ROIë¥¼ ì°¾ëŠ” ë°©ì‹
        - í˜„ì¬ ì½”ë“œëŠ” ì´ë¯¸ì§€ ì²˜ë¦¬ì™€ ì»¨íˆ¬ì–´ë¥¼ í™œìš©í•˜ì—¬ ë²ˆí˜¸íŒ ì˜ì—­ì„ íƒì§€
    - YOLO ë° EAST ëª¨ë¸ ë¯¸ì‚¬ìš©
        - ìƒˆ ì½”ë“œëŠ” YOLO ë° EASTì™€ ê°™ì€ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì§€ ì•Šê³ , ì´ë¯¸ì§€ ì „ì²˜ë¦¬ì™€ ì»¨íˆ¬ì–´ ê¸°ë°˜ íƒì§€ ë°©ì‹ì„ ì‚¬ìš©í•œë‹¤
        - ê²½ê³„ë©´ ë° ê±´íˆ¬ì–´ì˜ ë©´ì  ë“±ì„ í™œìš©í•´ ë²ˆí˜¸íŒ ì¶”ì¶œì„ ìˆ˜í–‰
    - ë²ˆí˜¸íŒ ROI ì¶”ì¶œ ë²”ìœ„ ì„¤ì •
        - ìƒˆ ì½”ë“œì—ì„œëŠ” ì´ë¯¸ì§€ ë‚´ë¶€ íŠ¹ì • ì˜ì—­(ìƒë‹¨ 50%~80%, ì¢Œìš° 30%~80%)ì„ ê¸°ì¤€ìœ¼ë¡œ ë²ˆí˜¸íŒ í›„ë³´ë¥¼ íƒìƒ‰í•œë‹¤
        - ì´ë ‡ê²Œ ì˜ì—­ ì¶•ì†Œë¥¼ í†µí•´ ì—°ì‚°ëŸ‰ì„ ì¤„ì¸ë‹¤
    - Tesseract ì„¤ì •
        - ìƒˆ ì½”ë“œì—ì„œëŠ” â€”psm 12 (í…ìŠ¤íŠ¸ ê°ì§€ìš© ë‹¨ì¼ í…ìŠ¤íŠ¸ í–‰)ê³¼ â€”oem 3 (ìµœì ì˜ OCR ëª¨ë¸ ì‚¬ìš©) ì„¤ì •ì„ ì‚¬ìš©í•œë‹¤
        - ì´ì „ ì½”ë“œì—ì„œëŠ” â€”psm 7 (ë‹¨ì¼ í…ìŠ¤íŠ¸ ë¼ì¸)ê³¼ â€”oem 1 (ë ˆê±°ì‹œì™€ ì‹ ê²½ë§ í˜¼í•©) ì„¤ì •ì„ ì‚¬ìš©
    - ì»¨íˆ¬ì–´ ê¸°ë°˜ íƒì§€
        - ì»¨íˆ¬ì–´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë²ˆí˜¸íŒ ì˜ì—­ì„ í•„í„°ë§í•˜ê³ , ì´ë¥¼ approxPolyDPë¡œ ë‹¤ê°í˜• ê·¼ì‚¬í™”ë¥¼ ìˆ˜í–‰í•˜ì—¬ ì‚¬ê°í˜•ë§Œ ì¶”ì¶œ
    
    ## ì½”ë“œ ì§„í–‰ ìˆœì„œ
    
    1. ì…ë ¥ ì´ë¯¸ì§€ë¥¼ ë¡œë“œ
    2. processROI
        1. ì´ë¯¸ì§€ë¥¼ HSVë¡œ ë³€í™˜í•˜ê³ , top-hat ë° black-hat ì—°ì‚°ìœ¼ë¡œ ì¡°ëª… íš¨ê³¼ ì œê±°
        2. Gaussian Blurì™€ Adaptive Thresholdë¡œ ë²ˆí˜¸íŒ í›„ë³´ ì˜ì—­ ê°•ì¡°
        3. íŠ¹ì • ì˜ì—­(ìƒë‹¨ 50%~80%, ì¢Œìš° 30%~80%)ë‚´ì—ì„œ ë²ˆí˜¸íŒ í›„ë³´ íƒìƒ‰
    <img width="692" alt="10" src="https://github.com/user-attachments/assets/3e5b5c15-9ea2-4b58-81e9-0dadab7d1831" />

           
            
        4. ì»¨íˆ¬ì–´ë¥¼ í•„í„°ë§í•˜ì—¬ ë²ˆí˜¸íŒ ROIë¥¼ ì¶”ì¶œ
            <img width="421" alt="11" src="https://github.com/user-attachments/assets/f5be5bd7-b110-4933-9751-2746ef290767" />

          
            
    4. textRead
        1. ë²ˆí˜¸íŒ ROIì—ì„œ Tesseract OCRë¡œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œ
        2. í…ìŠ¤íŠ¸ ê²°ê³¼ë¥¼ ì •ì œí•˜ì—¬ ì•ŒíŒŒë²³ ë° ìˆ«ìë§Œ ë‚¨ê¹€
    5. ë²ˆí˜¸íŒ í…ìŠ¤íŠ¸ì™€ ROIë¥¼ ì›ë³¸ ì´ë¯¸ì§€ì— í‘œì‹œ
        
     <img width="696" alt="12" src="https://github.com/user-attachments/assets/6c5e5dcd-6c6c-4772-b5c8-21902100a1ca" />

        
    6. ê²°ê³¼ë¥¼ í™”ë©´ì— ì¶œë ¥
     <img width="1105" alt="13" src="https://github.com/user-attachments/assets/a439adfe-313e-475a-959d-d1fe80ad3b77" />



        
